{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3675df-0829-400e-8d1b-c2899ce5e847",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Gradient-weighted Class Activation Mapping of Pneumonia Identification in Children by CNNs</p>\n",
    "##### This script provides visual explanations of the decisions made by CNNs for identification of pneumonia in children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffbd975e-25d3-42be-938d-8fee30102bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44fdfa09-93e7-4914-8eac-a060146b0ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained ResNet-18 model from a saved file\n",
    "model = torch.load('resnet18_model.pt', map_location=torch.device('cpu')) # switch to 'resnet18_preprocessed_model.pt'\n",
    "\n",
    "# set the model to evaluation mode, this is done during inference. Turns off certain behaviours used during training.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dda80965-2c66-48ef-893b-299a2de05841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet18(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A modified ResNet-18 model adapted for Grad-CAM analysis.\n",
    "\n",
    "    This class takes a pre-trained ResNet-18 model and modifies it to extraction feature maps \n",
    "    and gradients which are essential for Grad-CAM visualization. It retains all convolutional \n",
    "    layers of the original model except the last two (fully connected layers).\n",
    "\n",
    "    Attributes:\n",
    "        features (torch.nn.Sequential): The sequential container of all convolutional layers in the model.\n",
    "        pooling (torch.nn.Module): The average pooling layer from the original model.\n",
    "        fc (torch.nn.Module): The fully connected layer from the original model.\n",
    "        gradients (torch.Tensor): Tensor to store gradients of the activations.\n",
    "\n",
    "    Functions:\n",
    "        activations_hook(grad): Stores the gradients of activations during backpropagation.\n",
    "        forward(x): Defines the forward pass of the model.\n",
    "        get_activations_gradient(): Retrieves the stored gradients of activations.\n",
    "        get_activations(x): Extracts activations from the feature layers for a given input x.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        self.features = list(model.children())[:-2] # all layers except the last two\n",
    "        self.features = torch.nn.Sequential(*self.features)\n",
    "        self.pooling = model.avgpool\n",
    "        self.fc = model.fc\n",
    "        self.gradients = None\n",
    "      \n",
    "    def activations_hook(self, grad):\n",
    "        \"\"\"\n",
    "        Hook for capturing the gradients of the activations.\n",
    "        Required for: Gradient-based visualization techniques.\n",
    "\n",
    "        Args:\n",
    "            grad (Tensor): The gradient of the activations.\n",
    "\n",
    "        This method stores the gradient passed to it in the self.gradients attribute,\n",
    "        allowing for later retrieval and analysis of the gradients. \n",
    "        \"\"\"\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Required for: Computing the output of the model given an input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor to the model.\n",
    "\n",
    "        This method processes the input tensor through the modified feature layers,\n",
    "        registers a hook for gradient capturing, applies pooling, flattens the output,\n",
    "        and then passes it through the fully connected layer. This is needed for \n",
    "        feature extraction and gradient capture.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        \"\"\"\n",
    "        Retrieve the stored gradients of the activations.\n",
    "        Required for: Analyzing the gradients post-forward pass.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradients of the activations captured during the forward pass.\n",
    "\n",
    "        This method is useful for gradient-based analysis post model inference,\n",
    "        enabling insight into which parts of the input image the model is focusing on.\n",
    "        \"\"\"\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        \"\"\"\n",
    "        Retrieve the activations from the feature layers for a given input.\n",
    "        Required for: Extracting intermediate layer activations.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor for which activations need to be computed.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The activations from the feature layers of the model.\n",
    "\n",
    "        This method is useful for visualizing and analyzing the intermediate\n",
    "        features that the network learns and uses for making predictions.\n",
    "        \"\"\"\n",
    "        return self.features(x)\n",
    "\n",
    "modified_model = ModifiedResNet18(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89de0c42-b1df-4092-86d8-9d816a7a20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(modified_model, input_image, target_class):\n",
    "    \"\"\"\n",
    "    Computes a Grad-CAM heatmap for a specific target class using a modified CNN model.\n",
    "    \n",
    "    Performs a forward pass of the input image through the modified model to get the model's \n",
    "    output for the target class. Then, it executes a backward pass to calculate the gradients\n",
    "    of the target class output.These gradients are pooled and used to weight the activations \n",
    "    from the final convolutional layer of the model.By averaging these weighted activations \n",
    "    and applying a ReLU, a heatmap is generated. This heatmap visualizes the areas in the \n",
    "    input image that most significantly influence the model's prediction for the target class. \n",
    "    The resulting heatmap is a NumPy array, providing a visual interpretation of the model's\n",
    "    decision-making process for the class.\n",
    "\n",
    "    Args:\n",
    "        modified_model (torch.nn.Module): The modified neural network model.\n",
    "        input_image (Tensor): The input image for which the heatmap is to be generated.\n",
    "        target_class (int): The target class index for which the heatmap is to be computed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The computed Grad-CAM heatmap as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Forward\n",
    "    output = modified_model(input_image)\n",
    "    target = output[:, target_class]\n",
    "\n",
    "    # Backward\n",
    "    modified_model.zero_grad()\n",
    "    target.backward()\n",
    "\n",
    "    # Get the gradients and activations\n",
    "    gradients = modified_model.get_activations_gradient()\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    activations = modified_model.get_activations(input_image).detach()\n",
    "\n",
    "    # Weighting the activations with the gradients and creating heatmap\n",
    "    for i in range(activations.shape[1]):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= torch.max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31980622-a9ae-489f-9977-186412a99752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image.\n",
    "\n",
    "    Args:\n",
    "    - img_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Preprocessed image as a tensor.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')  # Convert image to RGB if it's not\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "target_class = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1079782f-5f87-41ed-b301-63b08e237c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_process(folder_path, model, target_class, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in a specified folder to apply Grad-CAM and save the resulting visualizations.\n",
    "    Required for: Automating the application of Grad-CAM to multiple images and saving the results.\n",
    "        - Iterating over all images in the specified folder.\n",
    "        - Preprocessing each image and applying the Grad-CAM function to generate a heatmap.\n",
    "        - Overlaying the heatmap on the original image and applying a colormap.\n",
    "        - Saving the overlaid image in the specified output folder.\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images to be processed.\n",
    "        model (torch.nn.Module): The neural network model, modified to capture gradients and activations.\n",
    "        target_class (int): The class index for which the Grad-CAM is to be applied to each image.\n",
    "        output_folder (str): Path to the folder where the Grad-CAM visualizations will be saved.\n",
    "\n",
    "    The function creates the output folder if it does not exist. \n",
    "    Each output file is named by prefixing 'gradcam_' to the original image filename.\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        if img_file.lower().endswith('.jpeg'):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            input_image = preprocess_image(img_path)\n",
    "            \n",
    "            # Apply Grad-CAM\n",
    "            heatmap = grad_cam(model, input_image, target_class)\n",
    "\n",
    "            # Convert the heatmap to PIL image and resize\n",
    "            overlay = to_pil_image(torch.from_numpy(heatmap), mode='F').resize(input_image.squeeze().shape[1:], resample=PIL.Image.BICUBIC)\n",
    "\n",
    "            # Apply a colormap to the heatmap\n",
    "            cmap = cm.jet\n",
    "            overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n",
    "\n",
    "            # Create a figure and plot the first image\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.axis('off')  # removes the axis markers\n",
    "\n",
    "            # First plot the original image\n",
    "            ax.imshow(to_pil_image(input_image.squeeze(), mode='RGB'))\n",
    "\n",
    "            # Plot the heatmap on the same axes with alpha < 1 (transparency)\n",
    "            ax.imshow(PIL.Image.fromarray(overlay), alpha=0.4, interpolation='nearest')\n",
    "\n",
    "            # Save the figure to the output folder\n",
    "            output_img_path = os.path.join(output_folder, 'gradcam_' + img_file)\n",
    "            plt.savefig(output_img_path, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0f49ce4-614f-48a4-b98a-be14003a03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_process(folder_path, model, target_class, output_folder):\n",
    "    \"\"\"\n",
    "    Processes a folder of images to generate Grad-CAM heatmaps, averages these heatmaps, \n",
    "    and saves the averaged heatmap to a specified output folder.\n",
    "\n",
    "    This function iterates over each JPEG image in the given folder, applies the Grad-CAM\n",
    "    algorithm to each image using the specified model and target class, and accumulates the\n",
    "    generated heatmaps. It then computes the average of these heatmaps, resizes the averaged \n",
    "    heatmap to the original image size, applies a colormap for better visualization, and saves\n",
    "    the result as an image in the output folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JPEG images to be processed.\n",
    "        model (torch.nn.Module): The neural network model, modified for Grad-CAM usage.\n",
    "        target_class (int): The class index for which Grad-CAM is applied to each image.\n",
    "        output_folder (str): Path to the folder where the averaged heatmap image will be saved.\n",
    "\n",
    "    The function assumes that the original images are of size 224x224 pixels. This size is used\n",
    "    when resizing the averaged heatmap. The output image is saved using a 'jet' colormap for\n",
    "    heatmap visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    heatmaps = []\n",
    "\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        if img_file.lower().endswith('.jpeg'):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            input_image = preprocess_image(img_path)\n",
    "            \n",
    "            # Apply Grad-CAM\n",
    "            heatmap = grad_cam(model, input_image, target_class)\n",
    "            heatmaps.append(heatmap)\n",
    "             \n",
    "    # Compute the average heatmap\n",
    "    average_heatmap = np.mean(heatmaps, axis=0)\n",
    "\n",
    "    # Convert the average heatmap to uint8 format\n",
    "    average_heatmap_uint8 = np.uint8(255 * average_heatmap / np.max(average_heatmap))\n",
    "\n",
    "    # Resize using PIL\n",
    "    original_image_size = (224, 224)\n",
    "    average_heatmap_resized = Image.fromarray(average_heatmap_uint8).resize(original_image_size, Image.BICUBIC)\n",
    "\n",
    "    # Apply colormap\n",
    "    average_heatmap_colored = cm.jet(np.array(average_heatmap_resized))[:, :, :3]\n",
    "    \n",
    "    # Save the average heatmap\n",
    "    output_img_path = os.path.join(output_folder, 'average_heatmap.png')\n",
    "    plt.imsave(output_img_path, average_heatmap_colored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac6abb5f-0613-4e17-9012-5c2c52f95b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image storage locations\n",
    "\n",
    "normal_test_preprocessed_folder = 'dataset/PRE_NORMAL/TEST'\n",
    "normal_train_preprocessed_folder = 'dataset/PRE_NORMAL/TRAIN'\n",
    "pneumonia_test_preprocessed_folder = 'dataset/PRE_PNEUMONIA/TEST'\n",
    "pneumonia_train_preprocessed_folder = 'dataset/PRE_PNEUMONIA/TRAIN'\n",
    "normal_test_unprocessed_folder = 'dataset/NORMAL/TEST'\n",
    "normal_train_unprocessed_folder = 'dataset/NORMAL/TRAIN'\n",
    "pneumonia_test_unprocessed_folder = 'dataset/PNEUMONIA/TEST'\n",
    "pneumonia_train_unprocessed_folder = 'dataset/PNEUMONIA/TRAIN'\n",
    "\n",
    "output_test_preprocessed_normal = 'output/PRE_NORMAL/TEST' \n",
    "output_train_preprocessed_normal = 'output/PRE_NORMAL/TRAIN' \n",
    "output_test_preprocessed_pneumonia = 'output/PRE_PNEUMONIA/TEST'\n",
    "output_train_preprocessed_pneumonia = 'output/PRE_PNEUMONIA/TRAIN'\n",
    "output_test_unprocessed_normal = 'output/NORMAL/TEST' \n",
    "output_train_unprocessed_normal = 'output/NORMAL/TRAIN' \n",
    "output_test_unprocessed_pneumonia = 'output/PNEUMONIA/TEST'\n",
    "output_train_unprocessed_pneumonia = 'output/PNEUMONIA/TRAIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a2756e1-2c56-488c-af6c-1ba3db7598fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions to produce average heatmaps\n",
    "\n",
    "heatmap_process(normal_test_preprocessed_folder, modified_model, target_class=1, output_folder=output_test_preprocessed_normal)\n",
    "heatmap_process(normal_train_preprocessed_folder, modified_model, target_class=1, output_folder=output_train_preprocessed_normal)\n",
    "\n",
    "heatmap_process(pneumonia_test_preprocessed_folder, modified_model, target_class=1, output_folder=output_test_preprocessed_pneumonia)\n",
    "heatmap_process(pneumonia_train_preprocessed_folder, modified_model, target_class=1, output_folder=output_train_preprocessed_pneumonia)\n",
    "\n",
    "heatmap_process(normal_test_unprocessed_folder, modified_model, target_class=1, output_folder=output_test_unprocessed_normal)\n",
    "heatmap_process(normal_train_unprocessed_folder, modified_model, target_class=1, output_folder=output_train_unprocessed_normal)\n",
    "\n",
    "heatmap_process(pneumonia_test_unprocessed_folder, modified_model, target_class=1, output_folder=output_test_unprocessed_pneumonia)\n",
    "heatmap_process(pneumonia_train_unprocessed_folder, modified_model, target_class=1, output_folder=output_train_unprocessed_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6a709-b602-4bf2-9cf2-c08bac5daeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions to produce GradCam images\n",
    "\n",
    "gradcam_process(normal_test_preprocessed_folder, modified_model, target_class=1, output_folder=output_test_preprocessed_normal)\n",
    "gradcam_process(normal_train_preprocessed_folder, modified_model, target_class=1, output_folder=output_train_preprocessed_normal)\n",
    "\n",
    "gradcam_process(pneumonia_test_preprocessed_folder, modified_model, target_class=1, output_folder=output_test_preprocessed_pneumonia)\n",
    "gradcam_process(pneumonia_train_preprocessed_folder, modified_model, target_class=1, output_folder=output_train_preprocessed_pneumonia)\n",
    "\n",
    "gradcam_process(normal_test_unprocessed_folder, modified_model, target_class=1, output_folder=output_test_unprocessed_normal)\n",
    "gradcam_process(normal_train_unprocessed_folder, modified_model, target_class=1, output_folder=output_train_unprocessed_normal)\n",
    "\n",
    "gradcam_process(pneumonia_test_unprocessed_folder, modified_model, target_class=1, output_folder=output_test_unprocessed_pneumonia)\n",
    "gradcam_process(pneumonia_train_unprocessed_folder, modified_model, target_class=1, output_folder=output_train_unprocessed_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f991c1c2-003c-4720-8e65-88243e1a66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_heatmap_on_lungs(lung_image_path, heatmap_folder, output_folder, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlays a fixed Grad-CAM heatmap onto a lung image and saves the result in the specified folder,\n",
    "    preserving the original colors of the heatmap.\n",
    "\n",
    "    Args:\n",
    "        lung_image_path (str): File path to the lung image.\n",
    "        heatmap_folder (str): Directory path where the Grad-CAM heatmap image is stored.\n",
    "        output_folder (str): Directory path where the overlaid image will be saved.\n",
    "        alpha (float, optional): Transparency of the heatmap overlay. Default is 0.4.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the fixed heatmap filename\n",
    "    heatmap_filename = 'average_heatmap_preprocessed_model.png'\n",
    "\n",
    "    # Generate the full path to the heatmap\n",
    "    heatmap_path = os.path.join(heatmap_folder, heatmap_filename)\n",
    "\n",
    "    # Load the lung image\n",
    "    lung_image = cv2.imread(lung_image_path)\n",
    "    lung_image = cv2.cvtColor(lung_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load the heatmap\n",
    "    heatmap = cv2.imread(heatmap_path, cv2.IMREAD_GRAYSCALE)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Resize heatmap to match lung image dimensions\n",
    "    heatmap = cv2.resize(heatmap, (lung_image.shape[1], lung_image.shape[0]))\n",
    "\n",
    "    # Manually blend the heatmap and lung image\n",
    "    overlayed_image = (1 - alpha) * lung_image + alpha * heatmap\n",
    "    overlayed_image = overlayed_image.astype(np.uint8)\n",
    "\n",
    "    # Convert back to PIL Image for saving\n",
    "    overlayed_image = Image.fromarray(overlayed_image)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Define a fixed output filename\n",
    "    output_filename = 'lung_with_heatmap_overlay.jpg'\n",
    "\n",
    "    # Save the image\n",
    "    output_img_path = os.path.join(output_folder, output_filename)\n",
    "    overlayed_image.save(output_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7a30e14-eae2-43ce-bb61-da1cf403eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions to produce average heatmap overlay on lung images \n",
    "\n",
    "overlay_heatmap_on_lungs('lung_NORMAL.jpeg', output_test_unprocessed_normal, output_test_unprocessed_normal)\n",
    "overlay_heatmap_on_lungs('lung_NORMAL.jpeg', output_test_preprocessed_normal, output_test_preprocessed_normal)\n",
    "overlay_heatmap_on_lungs('lung_NORMAL.jpeg', output_test_unprocessed_pneumonia, output_test_unprocessed_pneumonia)\n",
    "overlay_heatmap_on_lungs('lung_NORMAL.jpeg', output_test_preprocessed_pneumonia, output_test_preprocessed_pneumonia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
